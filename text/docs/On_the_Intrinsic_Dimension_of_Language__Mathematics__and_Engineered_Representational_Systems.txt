Title: On the Intrinsic Dimension of Language  Mathematics  and Engineered Representational Systems
Date:  2025-05-27
Source: On_the_Intrinsic_Dimension_of_Language__Mathematics__and_Engineered_Representational_Systems.pdf
On the Intrinsic Dimension of Language, Mathematics,
              and Engineered Representational Systems:
          A Manifold-Based Approach to Symbolic and Conceptual Spaces
                                                                                                  ∗
                                           Jason Agamemnon Sokaris
             Independent Researcher, Theoretical Neuroengineering and Quantum Information Systems
                                                  March 5, 2025
                                                      Abstract
            We investigate the concept of intrinsic dimension in linguistic and symbolic representations, focus-
        ing on how manifold-learning and dimension-reduction techniques can quantify the degrees of freedom
        underlying large corpora of text, formal mathematical notations, and programming languages. We
        draw analogies to physical theories, where low-dimensional spacetimes (e.g., 3 + 1) need not limit the
        potentially high-dimensional manifold structure of human or machine-coded symbol systems. We then
        outline methods for engineering languages or domain-specific notations that may reduce ambiguity,
        increase expressive power, and possibly aid in large-scale neural-network training. Practical pathways
        for implementing and testing these ideas are discussed, with an emphasis on mathematical rigor and
        computational feasibility rather than philosophical or ethical speculation.
Contents
1 Introduction                                                                                                                                                            2
2 Intrinsic Dimension in Data Manifolds                                                                                                                                   3
  2.1 Definition and Basic Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                   3
  2.2 Dimension Estimation Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                     3
3 Comparing Language, Mathematics,
  and Programming Languages                                                                                                                                               3
  3.1 Natural Language Manifolds . . . . . . .            .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   3
  3.2 Mathematics as a Symbolic System . . .              .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   4
  3.3 Programming Languages . . . . . . . . .             .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   4
  3.4 Qualitative Comparison and Key Insight              .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   4
4 Connection to Physical Dimensions                                                                                                                                       5
  4.1 Spacetime vs. Symbolic Manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                    5
  4.2 Minimal Descriptive Theories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                    5
  ∗
      Email: OneiroCybernetics@gmail.com
                                                              1
     On the Intrinsic Dimension of Language, Mathematics, and Engineered Representational Systems
5 Engineering Languages and Domain-Specific Notations                                                        5
  5.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   5
  5.2 Example: A Domain-Specific Math/Code Language . . . . . . . . . . . . . . . . . . . . . .              5
  5.3 Reducing Intrinsic Dimension via DSLs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        6
6 Impact on Large-Scale AI and Neural Scaling Laws                                                           6
  6.1 Scaling Laws in Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        6
  6.2 Using Engineered Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .       6
  6.3 Experimental Workflow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      6
7 Implementation Details for Dimension Analysis                                                              7
  7.1 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     7
  7.2 Dimension Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .       7
  7.3 Interpretation and Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        7
8 Conclusions and Future Directions                                                                          7
  8.1 Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    8
1     Introduction
In modern data analysis and machine learning, the notion of intrinsic dimension describes how a high-
dimensional dataset often lies on a lower-dimensional manifold, capturing the essential degrees of free-
dom [1–3]. Beyond computer vision and sensory data, natural language corpora also exhibit intrinsic
structure that may be significantly lower-dimensional than raw textual embeddings suggest. Likewise,
formal mathematical notations and programming languages can be seen as more syntactically constrained
manifolds within an even higher-dimensional raw symbol space.
    In parallel, physical theories are often succinctly encoded in low-dimensional constructs (e.g., La-
grangians in a 3 + 1 dimensional spacetime), yet this does not imply that human conceptual or linguistic
spaces must share the same dimension as physical spacetime. Instead, the dimension of a conceptual or
linguistic manifold hinges on combinatorial and structural degrees of freedom.
    This article aims to:
    1. Give a concise mathematical background on manifold-learning and dimension-reduction techniques,
       clarifying how intrinsic dimension is defined and estimated.
    2. Compare the approximate dimensions (in a manifold sense) of natural language, mathematics, and
       programming languages.
    3. Explore how engineered or domain-specific languages (DSLs) might reduce ambiguity and improve
       cognitive or computational efficiency.
    4. Draw connections to the dimension of spacetime in physics, clarifying why the dimension of conceptual
       or linguistic spaces is not necessarily limited by 3 + 1 geometry.
    5. Outline practical steps for researchers in computational linguistics, AI, and theoretical modeling to
       implement and test these ideas (e.g., dimension-estimation pipelines, DSL design).
  We intentionally omit discussions of philosophy or ethics, focusing instead on the engineering and
mathematical aspects necessary to prototype and evaluate dimension-based language design.
                                                       2
    On the Intrinsic Dimension of Language, Mathematics, and Engineered Representational Systems
2     Intrinsic Dimension in Data Manifolds
2.1     Definition and Basic Properties
Definition 1 (Intrinsic Dimension). Let M be a manifold embedded in a high-dimensional ambient space
RD . The intrinsic dimension of M is the smallest integer d such that M can be locally homeomorphic to
Rd over its extent [1, 3].
    In practice, for real-world datasets, M is often approximated numerically by various manifold-learning
or dimension-reduction techniques:
    • Linear Methods: Principal Component Analysis (PCA), Factor Analysis, etc.
    • Nonlinear Methods: Isomap, Locally Linear Embedding (LLE), Diffusion Maps, t-SNE, UMAP,
      etc. [2, 4].
    The effective dimension is sometimes determined by how many principal components (or nonlinear
modes) are needed to capture a chosen fraction (e.g., 95%) of the variance. Another approach is to
estimate fractal or correlation dimensions [5], but in text-based corpora, discrete or symbolic structures
often necessitate specialized techniques [6].
2.2     Dimension Estimation Algorithms
A variety of methods exist for estimating intrinsic dimension:
    1. PCA-based Methods: Plot cumulative variance versus number of principal components and locate
       the “elbow” or a threshold. This is simplistic but fast.
    2. Maximum Likelihood Estimation (MLE): Proposed by Levina and Bickel [7], using local neigh-
       borhoods to estimate dimensionality.
    3. Geometric or Fractal Approaches: E.g., correlation dimension, nearest-neighbor scaling, Minkowski-
       Bouligand dimension.
    4. Manifold-based Reductions (Isomap, LLE, UMAP): Dimensionality can be inferred from the
       embedding dimension yielding minimal reconstruction error.
    For large text corpora or symbolic strings, one typically embeds tokens or sentences into RD via word
embeddings (e.g., GloVe, Word2Vec), contextual embeddings (e.g., BERT, GPT), or specialized symbolic
feature engineering. Then, dimension estimation is applied to the resulting dataset of vectors {xi } ⊂ RD .
3     Comparing Language, Mathematics,
      and Programming Languages
3.1     Natural Language Manifolds
Natural languages (English, Mandarin, etc.) are vast, flexible, and replete with redundancies. Nevertheless,
modern large language models (LLMs) demonstrate that the distribution of valid sentences occupies a
lower-dimensional manifold within the space of all possible symbol sequences [8, 9].
   Empirical work [6] suggests:
    • The manifold dimension of typical corpora might be in the hundreds or low thousands, depending
      on how one measures.
                                                     3
   On the Intrinsic Dimension of Language, Mathematics, and Engineered Representational Systems
   • Language exhibits long-tail phenomena: rare words, idiomatic expressions, and varied styles that
     enlarge the overall manifold dimension.
   • Sub-languages (e.g., domain-specific medical or legal text) often restrict the dimension, focusing on
     specialized vocabulary and syntax.
3.2    Mathematics as a Symbolic System
Mathematical language is more rule-bound than natural language, with syntax governed by logical and
symbolic consistency. While there is no single measure of dimension for “all mathematics,” narrower
subfields or standardized notations often exhibit more constrained patterns.
   • Syntactic Consistency: Formulas must be well-formed according to grammars that define opera-
     tions, parentheses, function application, etc. This significantly prunes the space of possible strings.
   • Context-Free or Context-Sensitive Grammars: Formal definitions of mathematics (e.g., expres-
     sions in first-order logic) often are captured by context-free or mildly context-sensitive grammars [10].
   • Symbolic Reuse & Hierarchies: Many statements are re-expressions or transformations of existing
     lemmas. This reusability can reduce the effective dimension of new statements needed for typical
     mathematical communication.
   Hence, from a manifold perspective, one might argue that mathematics occupies a narrower subspace
than natural language. Empirical dimension estimates of large corpora of mathematical expressions (e.g.,
from arXiv preprints or theorem databases) could be attempted by extracting token embeddings (e.g.,
using specialized models like GPT-f [11]).
3.3    Programming Languages
Programming languages are typically even more constrained:
   • Formal Syntax and Semantics: Code must compile or interpret, implying a small fraction of all
     possible strings is valid.
   • Domain-Specific Patterns: Specialized APIs, libraries, or coding styles further constrain typical
     corpora.
   Recent neural code models (e.g., Codex, CodeT5) show that valid code corpora can still be large and
diverse, but dimension estimates often reveal strong concentration in certain patterns (function definitions,
loops, class structures) [12, 13].
3.4    Qualitative Comparison and Key Insight
         Table 1: Qualitative Comparison of Symbolic Systems by Approximate Constraining Factors
 System                             Constraints                     Approx. Dimension
 Natural Language        Semantics, grammar, usage norms        Large (hundreds to thousands)
 Mathematics           Strict symbolic rules, logic frameworks    Smaller or more structured
 Programming Languages  Very strict grammar, compiler rules    Potentially even more constrained
The key takeaway is that intrinsic dimension correlates inversely with how many syntactic and semantic
constraints exist. Natural language is flexible and ambiguous, mathematics is more precise, and program-
ming languages impose strict formality.
                                                      4
    On the Intrinsic Dimension of Language, Mathematics, and Engineered Representational Systems
4     Connection to Physical Dimensions
4.1    Spacetime vs. Symbolic Manifolds
Physical laws often operate in 3 + 1 dimensional spacetime (or in higher-dimensional scenarios postulated
by string theory, e.g., 10 or 11 dimensions) [14], but these geometric dimensions do not directly bound the
dimension of a data manifold representing linguistic or symbolic content.
Remark 1 (Distinction of Dimensions). The dimension of physical spacetime (e.g., 3 + 1) describes ge-
ometric degrees of freedom for particles or fields. In contrast, intrinsic dimension in a symbolic manifold
quantifies the degrees of freedom in sequences or expressions that are valid under a given grammar or usage.
The two notions are conceptually independent.
   Thus, while nature is described succinctly by low-dimensional frameworks (Hamiltonians, Lagrangians,
PDEs), the manifold of human or machine-generated statements about nature can be large-dimensional
due to linguistic redundancy and interpretive variety.
4.2    Minimal Descriptive Theories
In physics, a quest for minimal or elegant formulations (e.g., the principle of least action) might suggest
that there is a small set of fundamental rules. Yet, real-world data from scientific literature remains
high-dimensional because of expansions, approximations, domain-specific notations, and localized jargon.
   In principle, an engineered representation that stays close to fundamental physical laws—e.g., a lan-
guage directly encoding Lagrangians or functional integrals—could reduce redundancy. However, bridging
fundamental laws to everyday contexts typically reintroduces high-dimensional variability.
5     Engineering Languages and Domain-Specific Notations
5.1    Motivation
Designing new languages or domain-specific notations (DSLs) has a long history in mathematics, computer
science, and engineering [15, 16]. The goals often include:
    • Reducing Ambiguity: A more constrained grammar ensures fewer misinterpretations and errors.
    • Increasing Expressive Power: Carefully chosen primitives and abstractions allow concise and
      powerful statements.
    • Improving Learning Efficiency: Both humans and AI systems might learn and generalize more
      quickly if the data manifold is simpler (lower dimension or clearer topology).
5.2    Example: A Domain-Specific Math/Code Language
Consider a DSL for computational physics that merges partial differential equation (PDE) syntax with
discretization instructions. A minimal example:
                                Listing 1: DSL snippet for PDE specification
e q u a t i o n HeatEquation :
       domain = [ 0 , 1 ] x [ 0 , 1 ]
       PDE = dU/ dt = al pha ∗ ( dˆ2U/dx ˆ2 + dˆ2U/dy ˆ 2 )
        i n i t c o n d i t i o n = U( x , y , 0 ) = f ( x , y )
       b o u n d a r y c o n d i t i o n = U( x , 0 , t ) = g ( x , t )
                                                       5
     On the Intrinsic Dimension of Language, Mathematics, and Engineered Representational Systems
solver :
    method = f i n i t e d i f f e r e n c e
    g r i d r e s o l u t i o n = 100
   Here, the DSL grammar greatly restricts valid statements while also offering a concise expression of
PDE setup. Symbolically, this can drastically reduce the dimension of typical PDE inputs compared to
writing everything in a general-purpose language like English or C/C++.
5.3     Reducing Intrinsic Dimension via DSLs
From a manifold perspective, each valid DSL program is a point in some high-dimensional symbol space
ΣN , but the constraints ensure that feasible programs cluster in a low-dimensional subset:
                                               MDSL ⊂ ΣN .
Thus, the DSL effectively imposes strong topological constraints, lowering dimension while preserving (and
sometimes enhancing) expressive capacity in the target domain.
6     Impact on Large-Scale AI and Neural Scaling Laws
6.1     Scaling Laws in Language Models
Contemporary large-scale AI models (e.g., GPT-3, GPT-4, PaLM) exhibit consistent scaling laws: error
often decreases as a power law in model size (number of parameters), dataset size, and compute [17, 18].
However, diminishing returns appear once data distributions saturate a model’s capacity.
6.2     Using Engineered Representations
One hypothesis is that if the training data were expressed in a more dimensionally compact (less ambigu-
ous) form, the model might learn generalizations more efficiently. This is analogous to feeding a physics
engine well-structured PDE data instead of raw textual descriptions:
    1. Smaller Manifold Volume: Fewer irrelevant degrees of freedom in the input space.
    2. Lower Sample Complexity: The model needs fewer examples to converge on the correct patterns
       if the domain constraints are explicit.
    3. Better Generalization: A well-structured language enforces consistent patterns that a model can
       generalize from more systematically.
6.3     Experimental Workflow
    1. Construct DSL: For a target domain (e.g., mathematics, robotics, or physics-based simulations).
    2. Collect DSL Corpus: Generate or manually create a substantial dataset of valid programs or
       expressions in this DSL.
    3. Train LLM: Compare training efficiency and final performance on tasks (e.g., PDE solving, theorem
       proving) between:
         • Baseline: Large dataset in a less-structured or natural language format.
         • DSL: Equivalent dataset expressed in the engineered language.
                                                    6
     On the Intrinsic Dimension of Language, Mathematics, and Engineered Representational Systems
    4. Dimension Analysis: Perform manifold-learning or dimension-estimation techniques on both cor-
       pora to validate if the DSL indeed yields a lower-dimensional data manifold.
    5. Scaling Behavior: Evaluate whether the DSL approach shifts the power-law slope (improving or
       saturating at different rates).
    Such experiments allow direct, quantitative testing of whether language engineering can push or break
certain scaling barriers.
7     Implementation Details for Dimension Analysis
Below is a concise step-by-step pipeline for researchers aiming to measure and compare the dimension of
different symbolic corpora (natural language vs. DSL vs. formal math):
7.1     Data Preparation
    1. Tokenization/Parsing: Convert each text snippet (sentence, expression, code block) into a consis-
       tent token or parse-tree representation.
    2. Embedding: Map tokens to vectors in RD via pretrained embeddings or specialized language models
       (e.g., sentence-transformers or domain-specific embeddings).
7.2     Dimension Estimation
    1. Choose Local or Global Methods:
         • For large corpora, consider global approaches like PCA or Isomap on a sub-sampled set.
         • For local dimension, consider maximum likelihood estimators (MLE) [7].
    2. Evaluate Reconstruction Error: If using a nonlinear method (e.g., Isomap, LLE), embed the
       data in d dimensions for various d and track reconstruction error to find the “elbow.”
    3. Confidence Intervals: Use bootstrap or cross-validation to estimate variability in dimension esti-
       mates.
7.3     Interpretation and Comparison
    1. Compare Modalities: e.g., English text vs. a DSL for the same domain. If the DSL is effectively
       constraining the manifold, one expects a visibly lower dimension or simpler geometry.
    2. Downstream Task Performance: Correlate dimension with metrics such as model perplexity,
       accuracy, or sample complexity to see if a more compact manifold yields better or faster learning.
8     Conclusions and Future Directions
In this paper, we have:
    • Clarified the concept of intrinsic dimension in data manifolds and how it applies to language, math-
      ematics, and programming.
                                                    7
   On the Intrinsic Dimension of Language, Mathematics, and Engineered Representational Systems
   • Shown that physical spacetime dimension (e.g., 3 + 1) does not impose a direct cap on the dimension
     of symbolic manifolds, as the latter is governed by syntactic and semantic constraints rather than
     geometry.
   • Highlighted how engineering languages or domain-specific notations might reduce the effective di-
     mension and thereby aid both human cognition and large-scale AI training.
   • Provided a practical methodology for dimension estimation of symbolic corpora and proposed an
     experimental workflow to test whether dimension reduction in language can improve neural scaling
     performance.
8.1    Outlook
Potential next steps include:
  1. Systematic DSL Development: Create targeted DSLs for physics, finance, robotics, or theorem
     proving, then measure how it compares to standard English instructions.
  2. Hybrid Representations: Investigate merges between formal notations and contextual layers (like
     advanced macros in LATEX or specialized markup languages) to balance expressive freedom with
     structural constraints.
  3. Deep Integration in AI Systems: Incorporate dimension estimation directly into training loops
     as a regularization objective, favoring simpler or more compact representations to enhance general-
     ization.
Such lines of research may reveal how engineered representational systems can push the frontier of intelli-
gence—human or machine—beyond current scaling laws, all within a rigorous mathematical and compu-
tational framework.
Acknowledgments
The author thanks the open-source developer community for manifold-learning libraries and the many
researchers in domain-specific language design who have paved the way for structured symbolic systems.
References
 [1] J. A. Lee and M. Verleysen. Nonlinear Dimensionality Reduction. Springer, 2007.
 [2] J. B. Tenenbaum, V. d. Silva, and J. C. Langford. A global geometric framework for nonlinear
     dimensionality reduction. Science, 290(5500):2319–2323, 2000.
 [3] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science,
     290(5500):2323–2326, 2000.
 [4] L. van der Maaten and G. Hinton. Visualizing data using t-SNE. Journal of Machine Learning
     Research, 9:2579–2605, 2008.
 [5] P. Grassberger and I. Procaccia. Characterization of strange attractors. Physical Review Letters,
     50(5):346–349, 1983.
                                                    8
   On the Intrinsic Dimension of Language, Mathematics, and Engineered Representational Systems
 [6] A. Holtzman, J. Buys, M. Ouyang, D. Yin, and V. Z. Liang. Language drift: A theoretical framework
     and unified evaluation for semantic drift in natural language generation. In Proceedings of ACL, pages
     10989–11001, 2022.
 [7] E. Levina and P. J. Bickel. Maximum likelihood estimation of intrinsic dimension. In Advances in
     Neural Information Processing Systems, pages 777–784, 2005.
 [8] T. Brown et al. Language models are few-shot learners. In Advances in Neural Information Processing
     Systems, 2020.
 [9] Y. Liu et al. Roberta:      A robustly optimized BERT pretraining approach.           arXiv preprint
     arXiv:1907.11692, 2019.
[10] J. E. Hopcroft, R. Motwani, and J. D. Ullman. Introduction to Automata Theory, Languages, and
     Computation. Pearson, 2006.
[11] S. Polu and I. Sutskever. Generative language modeling for automated theorem proving. arXiv
     preprint arXiv:2009.03393, 2020.
[12] Z. Feng et al. Codebert: A pre-trained model for programming and natural languages. In Findings
     of EMNLP, pages 1536–1547, 2020.
[13] M. Chen et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374,
     2021.
[14] J. Polchinski. String Theory, Vols. 1 and 2. Cambridge University Press, 1998.
[15] P. Hudak. Building domain-specific embedded languages. ACM Computing Surveys, 28(4es), 1996.
[16] M. Fowler. Domain-Specific Languages. Addison-Wesley, 2010.
[17] J. Kaplan, S. McCandlish, T. Henighan, et al. Scaling laws for neural language models. arXiv preprint
     arXiv:2001.08361, 2020.
[18] J. Hoffmann et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556,
     2022.
                                                    9