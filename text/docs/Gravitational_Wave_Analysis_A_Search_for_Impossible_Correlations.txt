Title: Gravitational Wave Analysis A Search for Impossible Correlations
Date:  2025-05-27
Source: Gravitational_Wave_Analysis_A_Search_for_Impossible_Correlations.pdf
Blockchain & Gravitational-Wave Analysis:
             A Search for Impossible Correlations
                                       Your Name
                                      March 4, 2025
                                          Abstract
         In this paper, we discuss our exploratory journey analyzing the possible corre-
     lations between Bitcoin blockchain data (particularly block generation times) and
     gravitational-wave (GW) arrivals reported by LIGO. Motivated by a hypothesis in-
     volving manipulation of an immutable ledger and cosmic signals that theoretically
     cannot be altered, we have experimented with multiple advanced tools — including
     wavelet transforms, frequency-domain correlation, near/far set partitioning, and re-
     peated sampling. We present our methods and some preliminary observations here,
     emphasizing the usage of advanced statistical checks to detect “impossible” patterns
     that might suggest exotic phenomena such as time-travel tampering.
1    Introduction
Gravitational waves (GWs) are cosmic ripples in spacetime that propagate at the speed of
light, emanating from distant astrophysical events and detected by facilities such as LIGO.
The times at which GWs arrive on Earth are, in principle, invariant to any human action or
terrestrial technology.
    Meanwhile, the Bitcoin blockchain operates as a decentralized ledger with a consensus
mechanism intended to produce blocks at an average interval (e.g., ∼ 10 minutes), adjusting
the difficulty periodically to maintain that target rate. Under normal circumstances, we
should not expect the times at which blocks appear to have any statistically significant
correlation with GWs.
    The question arises: What if something manipulated the ledger or “rewrote” history in
a subtle way? Or, more whimsically, What if “time-travelers” attempted to embed signals
correlated with cosmic events? Our research impetus is to detect patterns or anomalies in the
blockchain’s block-generation data that should not exist if standard physics and consensus
rules alone are in effect.
                                              1
2     Data Acquisition & Preprocessing
2.1    Blockchain Data
We work with a CSV file, block_data_selected.csv, containing columns such as:
    • time (the timestamp parsed as datetime)
    • time_diff_seconds (the gap between this block and the previous)
    • difficulty (the network difficulty at time of mining)
    • ...
We sort these rows by time and often filter out segments prior to the earliest gravitational-
wave detection to ensure the data is somewhat consistent in network state (i.e., ignoring the
earliest Bitcoin era for simpler analysis).
2.2    Gravitational-Wave Events
From gw-all.json, we extract each gravitational-wave event’s GPS timestamp and convert
it to approximate UTC, ignoring leap seconds. We store these events as (UTC time, label)
pairs.
3     Analytical Methods
Throughout our exploration, we applied multiple scripts and approaches:
3.1    Near vs. Far Partitioning
A straightforward test is to label blocks that occur within ±∆t of any GW event as “GW-
block set” (or “near” set) and label blocks that occur at least ±2∆t from every GW event
as “far” set. We can then compare time_diff_seconds (or any other metric) in these two
sets:
    • Kolmogorov–Smirnov (KS) Test: checks for a distribution difference.
    • Welch’s t-test: checks if the means differ significantly.
    • Cohen’s d : estimates the effect size in standard deviations.
Under normal physics, we expect no difference. Repeated random sampling from the “far”
set ensures that any unexpected correlation is not a fluke.
                                               2
3.1.1    Repeated Runs
To verify stability of results, we repeat the near vs. far selection multiple times with different
random seeds, each time plotting normalized, log-scale histograms to visualize distribution
shapes. The script aggregates results and prints the p-values and Cohen’s d values for each
run, as well as a final combination histogram that merges all “far” samples across runs and
compares them to one consistent “GW” distribution.
3.2     Advanced Signal Overlap Checks
We explored more “sophisticated” techniques (some conceptual, some partially implemented):
    • Wavelet Decomposition & Coherence: Using pywt or a wavelet-coherence package
      to decompose the blockchain block intervals and a daily-binned GW “intensity” time
      series, thereby looking for suspicious coherence at certain scales.
    • Frequency-Domain (FFT) Correlation: Taking the FFT of the block interval time
      series and comparing it with the FFT magnitude of GW arrival counts, seeking shared
      peaks that might indicate periodic correlations.
    • Distribution Divergence: Checking if block intervals near GW events differ from
      the global distribution via the Kullback–Leibler divergence or other measures.
    • Machine-Learning Classification: In principle, training a classifier to predict whether
      a block was “near a GW event” from local features. A stable classification performance
      above random chance would be suspicious.
4       Example Code Excerpt
Below is a short excerpt from our repeated-run near/far partition script, demonstrating how
we test for differences:
                     Listing 1: Multi-run near/far partition script snippet.
# C o h e n s d f o r measuring e f f e c t s i z e
def c o h e n s d ( sample1 , sample2 ) :
    mean1 = np . mean ( sample1 )
    mean2 = np . mean ( sample2 )
    var1 = np . var ( sample1 , ddof =1)
    var2 = np . var ( sample2 , ddof =1)
    p o o l e d v a r = ( ( len ( sample1 ) −1)∗ var1 + ( len ( sample2 ) −1)∗ var2 ) / \
                          ( len ( sample1 ) + len ( sample2 ) − 2 )
    return ( mean1 − mean2 ) / np . s q r t ( p o o l e d v a r )
# ...
# f o r run i n range ( n r u n s ) :
#    # d e f i n e ’ g w v a l s ’ and ’ f a r v a l s ’ . . .
                                                  3
#   ks stat , ks pval           = ks 2samp ( g w v a l s , f a r v a l s )
#   t stat , t pval             = t t e s t i n d ( g w v a l s , f a r v a l s , e q u a l v a r=F a l s e )
#   d val                       = cohens d ( gw vals , f a r v a l s )
#   p r i n t ( f ”KS p={ k s   p v a l } , T p={ t p v a l } , Cohen ’ s d={ d v a l }”)
# ...
5      Observations & Preliminary Conclusions
    1. Normalization and Log Scale Are Crucial : When one set dwarfs the other or the shapes
       differ drastically, raw counts in a histogram can hide the smaller distribution. Using
       density=True (normed hist) and log-scaled y-axes helps highlight subtle differences.
    2. Repeated Sampling Can Provide Confidence: By resampling the “far” set multiple
       times, we see whether any difference persists or is merely a statistical anomaly in one
       random selection.
    3. Large p-values ̸= Confirmation, but small p-values + large Cohen’s d Might Indicate
       Something Odd : If we see consistent small p-values across repeated runs and a moder-
       ate or large effect size (Cohen’s d ≥ 0.5), that might be suspicious enough to warrant
       deeper investigation or more advanced modeling.
    4. More Advanced Methods: Potential Gains, Potential Pitfalls: Wavelet or frequency-
       domain checks are powerful, but the data’s inherent noise and the non-uniform spacing
       in block times can complicate direct interpretation. Machine learning might produce
       false positives if there are extraneous patterns in difficulty adjustments or daily cycles.
       Proper cross-validation and statistical rigor are necessary.
6      Future Work
We envision:
    • Permutation-based significance tests, ensuring that any found correlation is be-
      yond random labeling.
    • Refinement in wavelet coherence, better handling non-uniform sampling or using
      wavelet transforms specifically designed for event-based data.
    • In-depth neural or anomaly detection, to see if the ledger “fingerprint” near GW
      events is systematically different from normal times.
Ultimately, if no consistent anomalous correlation emerges, it reaffirms that the ledger is
unaffected by cosmic signals. But if repeated analyses find impossible correlations, it raises
interesting (though highly speculative) questions about “time-traveling tampering” or un-
known influences.
                                                   4
7     Conclusion
In summary, we have embarked upon an exploratory adventure, testing for “forbidden” cor-
relations between gravitational-wave arrivals and the Bitcoin blockchain’s block-generation
times. Though the premise is unconventional, the statistical and signal-processing ap-
proaches used here can be instructive for any scenario in which immutable data is to be
compared with invariant cosmic events.
    We present a set of methods and example scripts that:
    • Partition blocks into near vs. far from each GW event time.
    • Perform repeated random sampling and advanced statistical checks.
    • Offer wavelet, frequency, and distribution-based comparisons.
By combining these analyses with robust significance testing, we can systematically test for
correlations that, under normal physics, should not exist. Although the results so far do not
reveal conclusive evidence of “time-travel tampering,” the methodology stands ready to flag
suspicious anomalies should they ever arise.
Acknowledgments: Thanks to all open-source contributors for Python packages used (pan-
das, NumPy, Matplotlib, SciPy, PyWavelets, etc.) and to the broader gravitational-wave
community for making event data accessible.
References
[1] Nakamoto, S. (2008) Bitcoin:      A Peer-to-Peer Electronic Cash System. https://
    bitcoin.org/bitcoin.pdf
[2] LIGO Scientific Collaboration, Advanced LIGO. https://www.ligo.caltech.edu
                                             5